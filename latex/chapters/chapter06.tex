\chapter{Experimental Evaluation and Results}\label{chap:eval}
\section{Introduction}
The framework was evaluated using benchmark datasets, cloud telemetry, and adversary emulation scenarios. Metrics captured detection efficacy, latency, false positives, explainability, fairness, resource utilization, and compliance readiness.

\section{Datasets}
\begin{itemize}
    \item CIC-IDS~2017: network attacks for baseline benchmarking.
    \item UNSW-NB15: diverse modern traffic supporting generalization tests.
    \item DARPA OpTC: high-fidelity telemetry for lateral movement analysis.
    \item Cloud telemetry: logs from AWS, Azure, and Google Cloud sandboxes during simulated attacks S1--S5.
\end{itemize}

\section{Baseline Comparison}
\begin{table}[H]
\centering
\begin{tabular}{lccc}
\toprule
Metric & Baseline & Proposed & Improvement \\
\midrule
Precision & 0.84 & 0.93 & +10.7\% \\
Recall & 0.79 & 0.90 & +13.9\% \\
F1-score & 0.81 & 0.91 & +12.3\% \\
ROC-AUC & 0.87 & 0.95 & +9.2\% \\
MTTD (sec) & 92 & 68 & -26.1\% \\
False Positive Rate & 4.8\% & 3.1\% & -1.7 pp \\
\bottomrule
\end{tabular}
\caption{Baseline versus proposed framework performance.}
\end{table}

\section{Scenario Analysis}
Credential abuse, Kubernetes lateral movement, serverless exfiltration, crypto mining, and SaaS supply-chain scenarios demonstrated the framework's ability to correlate telemetry, raise timely alerts, and provide actionable explanations.

\section{Explainability and Analyst Feedback}
Analyst studies with twelve participants yielded a System Usability Scale score of 82 and a 24\% improvement in trust metrics compared with baseline dashboards. Counterfactual reasoning aided response planning.

\section{Fairness and Bias}
Performance variance across tenants and workloads remained within \pm3\%. Monitoring continues to detect potential class imbalance effects.

\section{Resource Utilization}
Training used four NVIDIA T4 GPUs for six hours per dataset. Ray Serve processed 5,000 events per second with 80~ms latency. Cloud cost for simulations was approximately USD~450.

\section{Compliance Validation}
Logging completeness met CERT-In retention requirements, automated incident reports were generated within four hours, and audit trails were exportable for regulators.

\section{Discussion}
The results support hypotheses H1--H4: hybrid AI improved accuracy, multi-telemetry fusion reduced detection latency, explainability raised analyst trust, and compliance orchestration operated without performance degradation. Limitations include reliance on lab telemetry and the need for broader real-world validation.
